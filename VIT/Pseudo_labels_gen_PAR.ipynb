{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f67dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import PAR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from time import time\n",
    "from skimage import color\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing as mp\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torchvision\n",
    "import torchsummary\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, EigenGradCAM, LayerCAM, FullGrad\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313f444f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "362ed512",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = '/home/zephyr/Desktop/Newcastle_University/11_FP_D/Dataset/Dataset/1.training/1.training'\n",
    "training_pseudo_labels = '/home/zephyr/Desktop/Newcastle_University/11_FP_D/Dataset/training_pseudo_labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba861f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img: np.ndarray, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], resized = (224, 224)) -> torch.Tensor:\n",
    "  \n",
    "  preprocessing = transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Resize((224, 224)),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                                      ])\n",
    "  return preprocessing(img.copy()).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c04f4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_transform(tensor, height=7, width=7):\n",
    "  result = tensor[:, 1 :  , :].reshape(tensor.size(0),\n",
    "                                       height, width, tensor.size(2))\n",
    "  \n",
    "  # Bring the channels to the first dimension,\n",
    "  # like in CNNs.\n",
    "  result = result.transpose(2, 3).transpose(1, 2)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c13770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_threshold(mask):\n",
    "    v = np.concatenate(mask)\n",
    "    t = v.mean()\n",
    "    d = np.inf\n",
    "    ds = 0.005\n",
    "    while d > ds:\n",
    "        g1 = v[v>t]\n",
    "        g2 = v[v<=t]\n",
    "        m1 = g1.mean()\n",
    "        m2 = g2.mean()\n",
    "        tp = (m1 + m2)/2\n",
    "        d = np.abs(t - tp)\n",
    "        t = tp\n",
    "        #print(t)\n",
    "\n",
    "    imt = mask > t\n",
    "\n",
    "    return imt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6057fc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vit_base_patch16_224 = timm.create_model('vit_base_patch32_224', pretrained = False, num_classes = 3,\n",
    "                                               drop_rate = 0.2, attn_drop_rate = 0.2).to(device)\n",
    "\n",
    "model_vit_base_patch16_224.load_state_dict(torch.load(f='/home/zephyr/Desktop/Newcastle_University/11_FP_D/Models/model_vit_base_patch32_224_2.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2461045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model_vit_base_patch16_224.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b06e3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LayerNorm((768,), eps=1e-06, elementwise_affine=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_layer = [model_vit_base_patch16_224.blocks[-1].norm1]\n",
    "target_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb331c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = GradCAM(model = model_vit_base_patch16_224, target_layers=target_layer,\n",
    "              reshape_transform=reshape_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60a5cc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PAR()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par = PAR.PAR(num_iter=15, dilations=[1,2,4,8,16,32])\n",
    "par.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afab8090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cam_refine(cam, image, image_tensor):\n",
    "    cam = par(image_tensor, transforms.ToTensor()(cam).unsqueeze(0))\n",
    "    cam = minmax_scale(cam.squeeze(0).squeeze(0).cpu().detach().numpy().reshape(-1, 1))\n",
    "    cam = cam.reshape(224, 224)\n",
    "    \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv2.threshold(gray, 210, 255, cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "    cam = cam*thresh\n",
    "    cam = mask_threshold(cam).astype(np.float32)\n",
    "    \n",
    "    return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ea1baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that loads images from a directory generates cam for them and saves them to a directory\n",
    "def generate_pseudo_labels(images_dir, pseudo_labels_dir, model, CAM):\n",
    "    for image_name in tqdm(os.listdir(images_dir), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}'):\n",
    "        image_path = os.path.join(images_dir, image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "        size = image.shape[:-1]\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "        image_tensor = preprocess_image(image)\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        #print(image_name)\n",
    "        label = list(map(int, image_name[-13:-4].strip('][').split(', ')))\n",
    "        \n",
    "        pseudo_label = np.zeros((224, 224, 4))\n",
    "        \n",
    "        #print(image_name)\n",
    "        \n",
    "        if label[0] == 1:\n",
    "            cam_0 = CAM(input_tensor=image_tensor, targets=[ClassifierOutputTarget(np.array([0]))])[0, :]\n",
    "            pseudo_label[:, :, 0] = cam_refine(cam_0, image, image_tensor)\n",
    "        \n",
    "        if label[1] == 1:\n",
    "            cam_1 = CAM(input_tensor=image_tensor, targets=[ClassifierOutputTarget(np.array([1]))])[0, :]\n",
    "            pseudo_label[:, :, 1] = cam_refine(cam_1, image, image_tensor)\n",
    "        \n",
    "        if label[2] == 1:\n",
    "            cam_2 = CAM(input_tensor=image_tensor, targets=[ClassifierOutputTarget(np.array([2]))])[0, :]\n",
    "            pseudo_label[:, :, 2] = cam_refine(cam_2, image, image_tensor)\n",
    "        \n",
    "        pseudo_label[:, :, 3] = 1 - pseudo_label.max(axis=-1)\n",
    "        \n",
    "        pseudo_label_path = os.path.join(pseudo_labels_dir, image_name)\n",
    "        pseudo_label = pseudo_label\n",
    "        \n",
    "        pseudo_label = cv2.resize(pseudo_label.astype(np.float32), size)\n",
    "        \n",
    "        cv2.imwrite(os.path.join(pseudo_labels_dir, image_name), pseudo_label*255)    # on loading the mask from the directory, normalize by dividing by 255 to get binary mask\n",
    "        #print(f'{image_name} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98e12318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24e0ce269bd469bbfc061bb8c0e216d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_119432/3965795295.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  m1 = g1.mean()\n",
      "/home/zephyr/.local/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_119432/3965795295.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  m1 = g1.mean()\n",
      "/home/zephyr/.local/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_119432/3965795295.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  m1 = g1.mean()\n",
      "/home/zephyr/.local/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_119432/3965795295.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  m1 = g1.mean()\n",
      "/home/zephyr/.local/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_119432/3965795295.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  m1 = g1.mean()\n",
      "/home/zephyr/.local/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_119432/3965795295.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  m1 = g1.mean()\n",
      "/home/zephyr/.local/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_119432/3965795295.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  m1 = g1.mean()\n",
      "/home/zephyr/.local/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_119432/3965795295.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  m1 = g1.mean()\n",
      "/home/zephyr/.local/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_119432/3965795295.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  m1 = g1.mean()\n",
      "/home/zephyr/.local/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_119432/3965795295.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  m1 = g1.mean()\n",
      "/home/zephyr/.local/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_119432/3965795295.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  m1 = g1.mean()\n",
      "/home/zephyr/.local/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_119432/3965795295.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  m1 = g1.mean()\n",
      "/home/zephyr/.local/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_pseudo_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_pseudo_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_vit_base_patch16_224\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcam\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36mgenerate_pseudo_labels\u001b[0;34m(images_dir, pseudo_labels_dir, model, CAM)\u001b[0m\n\u001b[1;32m     23\u001b[0m     pseudo_label[:, :, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m cam_refine(cam_1, image, image_tensor)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m     cam_2 \u001b[38;5;241m=\u001b[39m \u001b[43mCAM\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mClassifierOutputTarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     27\u001b[0m     pseudo_label[:, :, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m cam_refine(cam_2, image, image_tensor)\n\u001b[1;32m     29\u001b[0m pseudo_label[:, :, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m pseudo_label\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:184\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_smooth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_augmentation_smoothing(\n\u001b[1;32m    182\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:80\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     77\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [ClassifierOutputTarget(category) \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m target_categories]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muses_gradients:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([target(output) \u001b[38;5;28;01mfor\u001b[39;00m target, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(targets, outputs)])\n\u001b[1;32m     82\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1801\u001b[0m, in \u001b[0;36mModule.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1800\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1801\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_pseudo_labels(training_images, training_pseudo_labels, model_vit_base_patch16_224, cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c43eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09096dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
